1. 数据
   - tabular data表格类数据
   - 互相独立，互不影响
   - 离散型数据
     - zero-index
     - one-hot
   - 数字型特征:直接使用
   - 预处理
     - 中心化：以0为中心，正负都有 x-mu
     - 归一化: （x - min）/ (max - min)
     - 规范化： (x - mu)/ sigma
2. 模型
   - 分类：
     - KNN:惰性计算，训练很快(nothing to do),预测很慢, 简单好理解，规则加数据的非典型人工智能算法,准确率中等
     - 高斯朴素贝叶斯：利用贝叶斯公式，计算每个类别的概率，选择最大的。特征互相条件独立，高斯分布，使用概率密度函数代替概率，减少太多，训练时很快，预测时很快，准确率偏低
     - 决策树：利用了信息论中的熵（系统的混轮程度）的内涵，模型训练的过程，就是降低熵的过程，剪枝pruning算法(算法可以很复杂，也可以很简单。限制层数，每次分裂的最小样本数，样本越多，越容易构造出一颗复杂的树。如果不加限制，决策树会一直分裂，容易过拟合)，解释性比较好，树的每一次判断都清晰可见，可以对特征重要性进行排序，训练时速度中等，预测时很快，准确率比较高
       - 信息熵 -（P1*logP1 + P2*logP2 + ... +Pn*logPn）
       - Gini: - (p1 * (1 - p1) + p2*(1 - p2) + ... +pn*(1 - pn))
       - 方差
     - 支持向量机：最强个体！把事儿办了与把事情干好，训练时很慢，推理时很慢，准确率很好，适合于少样本，少特征！
       - 低维空间分不开的数据，映射到高维分开
     - 随机森林：集成学习，训练测试都比较快，准确率比较好
       - 随机：行级随机对样本进行了随机采样（bagging）,列级随机，最多sqrt(N)的特征
       - 森林：多棵决策树，default 100
       - bagging + voting的融合
     - 逻辑回归：深度学习中的二分类算法，打分函数！！看上去很二，其实很重要，sigmoid概率模拟
     - 集成学习：AdaBoost, GradientBoost, LightGBM, XGBoost
   - 回归: KNN, 线性回归，支持向量机，随机森林，决策树，集成学习
   - 聚类：
     - KMeans K均值算法
     - DBScan
   - 降维: PCA
3.流程
   - 先分析问题，搞定输入和输出
     - 输入有哪些特征？如何数字化？
     - 输出：分类，回归
   - 根据输入和输出，构造数据集
   - 遴选算法，完成输入到输出的映射
   - 模型评估，预测，上线
4.遗留问题：
   - 时序问题预测