### 循环神经网络
    - 目标：处理时序信号
    - 沿着某一个维度，有严格的顺序（前后依赖关系，前面决定后面）比如：股票，自然语言，语音信号等
    - 时序信号的处理方法：
        - 一维卷积 nn.Conv1d
        - CNN
            - 靠循环实现序列信号的抽取
            - SimpleRNN, LSTM长短期记忆网络, GRU
            - 优势：
                - 实现了统计机器学习到深度学习的过度！2006
                - 自然语言处理相关算法可以用于工程项目了！！！
            - 劣势：
                - 依赖循环，当序列很长时，无法并行计算！速度很慢，无法有效利用硬件加速！！
                - 通过隐藏状态来进行信息的传递，当序列很长时，信息会丢失！！
                - 模型搭建很多层时，没有得到足够的性能回报，要么很难收敛，要么性能没有提高多少（很少有超过三次的CNN）
        - 多头注意力机制
            - 优势：
                - 干掉循环！！可以并行提取特征，序列可以很长，可以处理1M上下文
                - 并行特征的抽取，没有前后顺序，也无从谈起信息遗忘！！！
                - 有RestBlock 和 LayerNorm的加持（梯度消失，梯度爆炸），模型可以无限堆叠！！重复越多，能力越大！！
                - 真正实现了量变到质变，能力发生了涌现
                - 什么是智能？面临新的场景，可以吧自己老的经验和知识迁移过来，解决新的问题
### Seq2Seq
    - sequence to sequence 序列转换模型
        - 进来一个序列，出去一个序列 text in, text out
        - Encoder2Decoder的一种特例
        - Seq Encoder 编码器：解析问句：将其编码为"中间表达向量"，本质：一个RNN（自动完成循环，一次性得到结果即可）
        - 中间表达 vector：承上启下，既是上句处理的结果，又是下句生成的参考，本质：RNN最后一个时间步的隐藏状态hn
        - Seq Decoder解码器：把中间表达作为初始化状态，逐个生成下句，本质：一个RNN（单步生成，自回归，手动循环）
        - 推理过程：自回归，一次生成一个词，把中间表达拿过来作为初始化隐藏状态,生产一个词输入下个，直到产生End信号
            - Start: Special token，控制符，本身有特殊内涵，控制模型的行为；训练是每句话的开始Start，结束加上End
        - 训练过程：优化技巧：空杯心态：X, y进来 把第一个正确答案直接输入第二步，加速训练过程,助力，teacher-forcing
    - CNN to CNN
        - UNet系列算法
    - CNN to RNN
        - 看图说话
        - Caption任务：加字幕
    - RNN to RNN
        - Seq2Seq NLP第一模型
        - 机器翻译
        - 文本生成
        - 语音识别
        - 文本摘要
### 问题思考
   - 如何实现长短不一的文本段落批量化训练？
     - 填充到相同长度时，如何消除填充词带来的噪声？
   - Encoder什么结构？有什么样的特色？
     - GRU,LSTM 没有任何特色
   - Decoder什么结构
     - GRU,LSTM 
     - 单步循环，实现自回归式预测！！
       - 训练时，有Teacher forcing机制，加快训练
    - 如何实现汉语到英语的翻译?
       - 逻辑是一样的，没什么差别