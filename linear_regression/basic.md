1. 如何求函数最小值？
    - 求解方法：
      - 第一步：求导
      - 第二步：另导数等于0
      - 第三步：解方程，求出极值点
      - 第四步：验证该点是否是极值点以及什么极值点
    - 上述方法的前提条件：
      - 可导，
      - 令（偏）导数=0，方程组可解
    - 换个思路
      - 随机选择一个初始点X0
      - 如果X0在最小值的左侧，则应该每次加上一个小正数，此时导数为负，则可以认为是减去导数
      - 如果X0在最小值的右侧，则应该每次减去一个小正数，此时导数为正，则可以认为是减去导数
      - 如果X0是最小值处，则导数为0，也可以认为是减去导数
    - 补充结论
      - 一元函数，称为导数；多元函数称为偏导数，也就是梯度
      - 减去导数=减去梯度，这就是梯度下降！！！！！！（先求梯度，然后让梯度下降）
2. 理论数学与工程数学的区别：如果函数有9B变量，可以迭代实现，使用GPU
3.  感知机 Perception， 一层全连接， 稠密层: 相乘再相加过程，同时是向量求内积的过程
4. 特别说明
   - 在训练时，X，y 是训练集中的特征和标签，看作常量； w和b是变量，待优化的值
   - 在推理时， w和b已经找到了比较合适的值，已经固定下来了，看作常量，此时，x是待预测样本的特征，是变量；预测的本质就是把x带入，求解y
   - 如何训练？有监督训练的过程：
     - 从训练集中取出一对x和y
     - 把x带入模型，求解预测结果y_pred
     - 想办法度量y_pred与y的误差loss
     - 特别说明：loss是y_pred与y相关的函数，y_pred是模型预测的结果，是w和b的函数；简单来说loss也是w和b的函数！！
     - 训练的本质求解何时loss函数最小，不断让loss减少的过程：当w和b取得什么值时，loss最小！！！求loss函数最小值问题
     - 
     - 
5. 深度学习的基本流程
   - 模型 model:
     - 只负责正向传播forward propagation，不负责反向传播 backward propagation
     - 正向传播：把特征X带入模型model，得到预测结果y_pred
       - 训练时：自动在底层构建计算图（把正向传播的流程记录下来，方便进行后续的分布求导）
       - h(g(f(x))) =f'g'h',按层更容易求导，分步求导？？？，也叫链式求导
       - 推理时：直接调用正向传播即可，不需要构建计算图
     - 反向传播：本质是计算每个参数的梯度，是通过loss函数发起的！！！
   - 训练流程：
     - 从训练集中取一批batch样本（x, y）
     - 把样本x送入模型model，得到预测结果y_pred
     - 通过损失函数，计算当前误差 loss = f(y_pred, y)
     - 通过loss, 反向传播，计算每个参数（w, b）的梯度
     - 利用优化器optimizer,通过梯度下降法，更新参数
     - 利用优化器optimizer 清空参数梯度
     - 重复以上过程，直至迭代结束（各项指标满足要求）
   - 推理流程
     - 拿到待测样本X（推理时，没有标签，只有特征）
     - 把样本特征X送入模型model,得到预测结果y_pred
     - 根据y_pred解析并返回预测结果
6.深度学习的项目流程
     - 批量化打包数据
       - 使用生成器来打包数据，通过某种规则调用数据，流式处理
       - 实现自定义dataset, 再定义dataloader
     - 搭建模型
       - sequential序贯模型，顺序模型（每个层都可看作一个模型）
       - class:自定义模型
     - 定义损失函数和优化器
       - 损失函数：MSE和CE
       - 优化器：SGD， Adam
     - 训练模型
       - 训练过程查看监控指标：如准确率，损失率
       - 训练过程中需要保持模型参数，方便后续推理
       - 避免过拟合
     - 推理模型
       - 带入
7.回调函数
     - 回调函数分为定义和调用两个过程
       - 由用户定义，由系统来调用
       - 你定义一个函数，挂载一个地方，系统在合适的时机，调用它
       - 一般来说，回调函数都不应该又用户主动调用！！