1.先检测地板砖，用目标检测地板砖，改YOLO模型参数，0.33
2.切图
3.tensorRT推理框架

法国UTAC
厦门ABB
唐宁公司Corning

学习率0.1一般太高了

Adam更稳定
大模型2M的batch size
CV调参内功修炼，学术研究；主要还是搞数据
大模型不再调参
YOLO可以设置几轮之内没有提升就停止
多任务处理：特征抽取+多任务输出


AIGC:
PAI-DSW
pip install -e .[metrics] 
pip install tf-keras
llamafactory-cli webui

DPO是 PPO升级 偏好矫正;区别？
Pre-traning 无底洞，几乎应用层不会做

ModelScope: TeleChat-PTD:电信语料库

nvidia-smi -l 2

全参微调需要的资源更大，但训练更快更好
一般损失调到0.5就差不多了

预训练数据集：c4_, wiki
微调数据集？？identity,apala_zh_demo


RLHF+PPO两阶段：ChatGPT玩法，现在直接用DPO偏好矫正
第三个阶段RLHF偏好纠正，一般工作中不用做，纠正现有模型偏好：回答的问题不太好，需要纠正

RLHF lora：打分函数，损失小打分很高说明答案很好，奖励+PPO lora+sft数据集
DPO lora微调补丁怎么用的，与原模型一起Export，补丁不能直接使用
full 训练后的模型可以直接使用
DPO：直接训练偏好数据

vllm启动服务：
python -m vllm.entrypoints.openai.api_server --model Qwen2-1.5B-Instruct --host 127.0.0.1 --port 8080
langchain上层开发
langchain, vllm???


XInference Embedding 方式

部署：百度智能云千帆
阿里云

Embedding模型:API
常规模型:base, chat


langchai API*** 画图整理

langSmith 监控中间过程
langfuse汉化平替，数据安全问题

1.大模型辅助业务，分析原有业务系统，哪些是可以用大模型处理的
2.无痛迁移
3.大模型落地比较宽容

注册账号：百度智能云，快速学习上层开发；注册生成API KEY
阿里云->阿里云百炼 通过langchain做上层开发
质朴BigModel
kimikimi


docker build -t qwen2-vllm-langchain-app .
